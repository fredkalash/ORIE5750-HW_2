{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "1    500\n",
      "0    500\n",
      "Name: 1, dtype: int64\n",
      "(1000, 2)\n",
      "1    500\n",
      "0    500\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "yelp_df = pd.read_csv(\"yelp_labelled.txt\", sep='\\t', header=None)\n",
    "print(yelp_df.shape)\n",
    "print(yelp_df[1].value_counts())\n",
    "amazon_df = pd.read_csv(\"amazon_cells_labelled.txt\", sep='\\t', header=None)\n",
    "print(amazon_df.shape)\n",
    "print(amazon_df[1].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# This fails with (748,2) instead of (1000,2)\n",
    "#imdb_df = pd.read_table(\"imdb_labelled.txt\", sep='\\t', header=None, \n",
    "#                      encoding='utf_8',verbose=True)\n",
    "#print(imdb_df.shape)\n",
    "\n",
    "# using readlines()\n",
    "count = 0 \n",
    "text_list = []\n",
    "score_list = []\n",
    "with open(\"imdb_labelled.txt\") as txt_file:\n",
    "    lines = txt_file.readlines()\n",
    "    for line in lines:\n",
    "        split_line = line.split('\\t')\n",
    "        text_list.append(split_line[0])\n",
    "        score_list.append(split_line[1])\n",
    "        count += 1\n",
    "print(count)\n",
    "print(len(text_list))\n",
    "print(len(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All in all its an insult to one's intelligence and a huge waste of money.  \t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels are all perfectly balanced for all files. 500:500 for each\n",
    "#file. 1:1. \n",
    "#I processed the files via Pandas and Python readlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'asdfasdf_whatever': 1, 'whatever_hello': 1})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "#import nltk\n",
    "\n",
    "STOPWORDS_SET = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once',\n",
    "                 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for',\n",
    "                 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's',\n",
    "                 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below',\n",
    "                 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more',\n",
    "                 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to',\n",
    "                 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and',\n",
    "                 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what',\n",
    "                 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has',\n",
    "                 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom',\n",
    "                 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further',\n",
    "                 'was', 'here', 'than',''}\n",
    "TUP_FLAG = 1\n",
    "def preprocess_text(input_text):\n",
    "    \"\"\"\n",
    "    Args: input_text (str)\n",
    "    Returns: word_count (defaultdict)\n",
    "    \"\"\"\n",
    "    # Lowercase all letters because dataset size is only 3000.\n",
    "    input_text = input_text.lower()\n",
    "    \n",
    "    # Do not lmennatization because we will use 2 tuples later.\n",
    "    # Strip punctuation because of often repeated !!!\n",
    "    input_text = re.sub(r'[^\\w\\s]', '', input_text)\n",
    "    \n",
    "    # Take out stop words because we aren't using TFIDF here\n",
    "    # Might mess up 2 tuple ngram model later\n",
    "    wc_dict = defaultdict(int)\n",
    "    split_text = input_text.split(' ')\n",
    "    if not TUP_FLAG:\n",
    "        for word in split_text:\n",
    "            if word not in STOPWORDS_SET:\n",
    "                wc_dict[word] += 1\n",
    "    else:\n",
    "        filtered_words = [word for word in split_text if word not in STOPWORDS_SET]\n",
    "        if len(filtered_words) < 2:\n",
    "            return wc_dict\n",
    "        else:          \n",
    "            for i in range(len(filtered_words)-1):\n",
    "                word = filtered_words[i] + '_' + filtered_words[i+1]\n",
    "                wc_dict[word] += 1\n",
    "    return wc_dict\n",
    "\n",
    "tmp = preprocess_text('asdfASDF.! whatever, ourselves, do its and, so any them before, hello how are you')\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fails with (748,2) instead of (1000,2)\n",
    "#imdb_df = pd.read_table(\"./sentiment labelled sentences/imdb_labelled.txt\", sep='\\t', header=None, \n",
    "#                      encoding='utf_8',verbose=True)\n",
    "#print(imdb_df.shape)\n",
    "TUP_FLAG = 0\n",
    "# using readlines()\n",
    "\n",
    "\n",
    "def nlp_train_test_split(file_name=\"imdb_labelled.txt\"):\n",
    "    count = 0 \n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    train_text_list = []\n",
    "    train_score_list = []\n",
    "    test_text_list = []\n",
    "    test_score_list = []\n",
    "    with open(file_name) as txt_file:\n",
    "        lines = txt_file.readlines()\n",
    "        for line in lines:\n",
    "            count += 1\n",
    "            split_line = line.split('\\t')\n",
    "            target_val = int(split_line[1])\n",
    "            text_val = preprocess_text(split_line[0])\n",
    "            if target_val == 0:\n",
    "                neg_count +=1 \n",
    "            else:\n",
    "                pos_count +=1         \n",
    "            if target_val == 0 and neg_count <= 400:\n",
    "                train_score_list.append(target_val)\n",
    "                train_text_list.append(text_val)\n",
    "            elif target_val == 1 and pos_count <= 400:\n",
    "                train_score_list.append(target_val)\n",
    "                train_text_list.append(text_val)\n",
    "            elif target_val == 0 and neg_count > 400:\n",
    "                test_score_list.append(target_val)\n",
    "                test_text_list.append(text_val)            \n",
    "            elif target_val == 1 and pos_count > 400:\n",
    "                test_score_list.append(target_val)\n",
    "                test_text_list.append(text_val)    \n",
    "    return train_text_list, train_score_list, test_text_list, test_score_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_list = [\"imdb_labelled.txt\",\n",
    "             \"yelp_labelled.txt\",\n",
    "             \"amazon_cells_labelled.txt\",\n",
    "            ]\n",
    "train_text_list = []\n",
    "train_score_list = []\n",
    "test_text_list = []\n",
    "test_score_list = []\n",
    "for file_name in file_list:\n",
    "    tr1,tr2, te1, te2 = nlp_train_test_split(file_name)\n",
    "    train_text_list += tr1\n",
    "    train_score_list += tr2\n",
    "    test_text_list += te1\n",
    "    test_score_list += te2\n",
    "    \n",
    "print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "600\n",
      "2400\n",
      "600\n",
      "1    1200\n",
      "0    1200\n",
      "dtype: int64\n",
      "1    300\n",
      "0    300\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text_list))\n",
    "print(len(test_text_list))\n",
    "print(len(train_score_list))\n",
    "print(len(test_score_list))\n",
    "\n",
    "print(pd.Series(train_score_list).value_counts())\n",
    "print(pd.Series(test_score_list).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int,\n",
       "             {'slowmoving': 1,\n",
       "              'aimless': 1,\n",
       "              'movie': 1,\n",
       "              'distressed': 1,\n",
       "              'drifting': 1,\n",
       "              'young': 1,\n",
       "              'man': 1}),\n",
       " defaultdict(int,\n",
       "             {'sure': 1,\n",
       "              'lost': 1,\n",
       "              'flat': 1,\n",
       "              'characters': 1,\n",
       "              'audience': 1,\n",
       "              'nearly': 1,\n",
       "              'half': 1,\n",
       "              'walked': 1}),\n",
       " defaultdict(int,\n",
       "             {'attempting': 1,\n",
       "              'artiness': 1,\n",
       "              'black': 1,\n",
       "              'white': 1,\n",
       "              'clever': 1,\n",
       "              'camera': 1,\n",
       "              'angles': 1,\n",
       "              'movie': 1,\n",
       "              'disappointed': 1,\n",
       "              'became': 1,\n",
       "              'even': 1,\n",
       "              'ridiculous': 1,\n",
       "              'acting': 1,\n",
       "              'poor': 1,\n",
       "              'plot': 1,\n",
       "              'lines': 1,\n",
       "              'almost': 1,\n",
       "              'nonexistent': 1}),\n",
       " defaultdict(int, {'little': 1, 'music': 1, 'anything': 1, 'speak': 1}),\n",
       " defaultdict(int,\n",
       "             {'best': 1,\n",
       "              'scene': 1,\n",
       "              'movie': 1,\n",
       "              'gerardo': 1,\n",
       "              'trying': 1,\n",
       "              'find': 1,\n",
       "              'song': 1,\n",
       "              'keeps': 1,\n",
       "              'running': 1,\n",
       "              'head': 1})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int, {'awkward': 1, 'use': 1, 'unreliable': 1}),\n",
       " defaultdict(int, {'good': 1, 'hoped': 1})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_list[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4590\n"
     ]
    }
   ],
   "source": [
    "vocabulary_dict = defaultdict(int)\n",
    "\n",
    "for d in train_text_list:\n",
    "    for k,v in d.items():\n",
    "        vocabulary_dict[k] += v\n",
    "\n",
    "print(len(vocabulary_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can't use test at the start because in production ML you don't\n",
    "#have access to future data (e.g. test)\n",
    "#Therefore your model can't access the future word counts in test.\n",
    "#These are unknown in production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2400x99967 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14586 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "import hashlib \n",
    "\n",
    "hash_word_dict = {}\n",
    "MAX_COLUMNS = 99999 #2147483646 # possible hash collisions\n",
    "data_list = []\n",
    "row_ind_list = []\n",
    "col_ind_list = []\n",
    "\n",
    "for i,d in enumerate(train_text_list):\n",
    "    for k,v in d.items():\n",
    "        row_ind_list.append(i)\n",
    "        data_list.append(np.log1p(v))\n",
    "        val = int(hashlib.sha256(k.encode()).hexdigest(),16) % MAX_COLUMNS\n",
    "        hash_word_dict[val] = k\n",
    "        col_ind_list.append(val)\n",
    "#You can create csr_matrix at once (like this format: csr_matrix((data, (row_ind, col_ind))). \n",
    "X_train = sp.csr_matrix((data_list, (row_ind_list, col_ind_list))) # sparse csr matrix\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<600x99967 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2957 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_COLUMNS = 99999 #2147483646\n",
    "data_list = []\n",
    "row_ind_list = []\n",
    "col_ind_list = []\n",
    "\n",
    "for i,d in enumerate(test_text_list):\n",
    "    for k,v in d.items():\n",
    "        if vocabulary_dict[k] > 0:\n",
    "            row_ind_list.append(i)\n",
    "            data_list.append(np.log1p(v))\n",
    "            val = int(hashlib.sha256(k.encode()).hexdigest(),16) % MAX_COLUMNS\n",
    "            col_ind_list.append(val)\n",
    "#You can create csr_matrix at once (like this format: csr_matrix((data, (row_ind, col_ind))). \n",
    "X_test = sp.csr_matrix((data_list, (row_ind_list, col_ind_list))) # sparse csr matrix\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.000001)\n",
    "clf.fit(X_train, train_score_list)\n",
    "preds = clf.predict(X_test)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[247,  53],\n",
       "       [ 81, 219]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_val =  confusion_matrix(test_score_list, preds)\n",
    "cm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_naive_bayes():\n",
    "    \"\"\"\n",
    "    Scipy implementation of MultinomialNB with equal class distribution assumed. \n",
    "    Our example in the HW2 has equal class distribution 1:1\n",
    "    \"\"\"\n",
    "    y = sp.csr_matrix(([1]*len(train_score_list), (list(range(len(train_score_list))),\n",
    "                                           train_score_list)))\n",
    "\n",
    "    feature_count = y.T.dot(X_train)\n",
    "    preds =  feature_count.dot(X_test.T)\n",
    "    \n",
    "    # If class distribution was not 1:1\n",
    "    #print(preds.shape)\n",
    "    mean_val = np.mean(train_score_list)\n",
    "    if mean_val != 0.5:\n",
    "        class_prior = np.vstack((np.array([1-mean_val]*preds.shape[1]),\n",
    "                                np.array([mean_val]*preds.shape[1])))\n",
    "        #print(class_prior.shape)\n",
    "        preds += np.log1p(class_prior.reshape(2,-1))\n",
    "    \n",
    "    \n",
    "    preds = np.argmax(preds, axis=0)\n",
    "    return preds.tolist()[0]\n",
    "\n",
    "preds = my_naive_bayes()\n",
    "preds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will use log-normalization because of the sparseness of the data.\n",
    "#I want to Laplace smooth the counts for naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[226,  74],\n",
       "       [ 72, 228]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_val =  confusion_matrix(test_score_list, preds)\n",
    "cm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7566666666666667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MyNaiveBayes Test accuracy of 0.75666666666667\n",
    "np.sum(np.array(test_score_list)== np.array(preds))/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[261,  39],\n",
       "       [ 87, 213]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty='elasticnet',l1_ratio=0.5, solver='saga')\n",
    "clf.fit(X_train, train_score_list)\n",
    "preds = clf.predict(X_test)\n",
    "print(np.sum(np.array(test_score_list)== np.array(preds))/len(preds))\n",
    "#0.7883333333333333\n",
    "cm_val =  confusion_matrix(test_score_list, preds)\n",
    "cm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_word_dict[np.argmax(clf.coef_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_word_dict[np.argmin(clf.coef_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive words such as great make a label great, Negative words such\n",
    "#as bad make a label bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray([[226,  74],\\n       [ 72, 228]])\\nTest Accuracy 0.7583333333333333\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "array([[226,  74],\n",
    "       [ 72, 228]])\n",
    "Test Accuracy 0.7566666666666667\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUP_FLAG = 1\n",
    "\n",
    "\n",
    "\n",
    "def nlp_train_test_split(file_name=\"imdb_labelled.txt\"):\n",
    "    count = 0 \n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    train_text_list = []\n",
    "    train_score_list = []\n",
    "    test_text_list = []\n",
    "    test_score_list = []\n",
    "    with open(file_name) as txt_file:\n",
    "        lines = txt_file.readlines()\n",
    "        for line in lines:\n",
    "            count += 1\n",
    "            split_line = line.split('\\t')\n",
    "            target_val = int(split_line[1])\n",
    "            text_val = preprocess_text(split_line[0])\n",
    "            if target_val == 0:\n",
    "                neg_count +=1 \n",
    "            else:\n",
    "                pos_count +=1         \n",
    "            if target_val == 0 and neg_count <= 400:\n",
    "                train_score_list.append(target_val)\n",
    "                train_text_list.append(text_val)\n",
    "            elif target_val == 1 and pos_count <= 400:\n",
    "                train_score_list.append(target_val)\n",
    "                train_text_list.append(text_val)\n",
    "            elif target_val == 0 and neg_count > 400:\n",
    "                test_score_list.append(target_val)\n",
    "                test_text_list.append(text_val)            \n",
    "            elif target_val == 1 and pos_count > 400:\n",
    "                test_score_list.append(target_val)\n",
    "                test_text_list.append(text_val)    \n",
    "    return train_text_list, train_score_list, test_text_list, test_score_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_list = [\"imdb_labelled.txt\",\n",
    "             \"yelp_labelled.txt\",\n",
    "             \"amazon_cells_labelled.txt\",\n",
    "            ]\n",
    "train_text_list = []\n",
    "train_score_list = []\n",
    "test_text_list = []\n",
    "test_score_list = []\n",
    "for file_name in file_list:\n",
    "    tr1,tr2, te1, te2 = nlp_train_test_split(file_name)\n",
    "    train_text_list += tr1\n",
    "    train_score_list += tr2\n",
    "    test_text_list += te1\n",
    "    test_score_list += te2\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "600\n",
      "2400\n",
      "600\n",
      "1    1200\n",
      "0    1200\n",
      "dtype: int64\n",
      "1    300\n",
      "0    300\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text_list))\n",
    "print(len(test_text_list))\n",
    "print(len(train_score_list))\n",
    "print(len(test_score_list))\n",
    "\n",
    "print(pd.Series(train_score_list).value_counts())\n",
    "print(pd.Series(test_score_list).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int,\n",
       "             {'slowmoving_aimless': 1,\n",
       "              'aimless_movie': 1,\n",
       "              'movie_distressed': 1,\n",
       "              'distressed_drifting': 1,\n",
       "              'drifting_young': 1,\n",
       "              'young_man': 1}),\n",
       " defaultdict(int,\n",
       "             {'sure_lost': 1,\n",
       "              'lost_flat': 1,\n",
       "              'flat_characters': 1,\n",
       "              'characters_audience': 1,\n",
       "              'audience_nearly': 1,\n",
       "              'nearly_half': 1,\n",
       "              'half_walked': 1}),\n",
       " defaultdict(int,\n",
       "             {'attempting_artiness': 1,\n",
       "              'artiness_black': 1,\n",
       "              'black_white': 1,\n",
       "              'white_clever': 1,\n",
       "              'clever_camera': 1,\n",
       "              'camera_angles': 1,\n",
       "              'angles_movie': 1,\n",
       "              'movie_disappointed': 1,\n",
       "              'disappointed_became': 1,\n",
       "              'became_even': 1,\n",
       "              'even_ridiculous': 1,\n",
       "              'ridiculous_acting': 1,\n",
       "              'acting_poor': 1,\n",
       "              'poor_plot': 1,\n",
       "              'plot_lines': 1,\n",
       "              'lines_almost': 1,\n",
       "              'almost_nonexistent': 1}),\n",
       " defaultdict(int,\n",
       "             {'little_music': 1, 'music_anything': 1, 'anything_speak': 1}),\n",
       " defaultdict(int,\n",
       "             {'best_scene': 1,\n",
       "              'scene_movie': 1,\n",
       "              'movie_gerardo': 1,\n",
       "              'gerardo_trying': 1,\n",
       "              'trying_find': 1,\n",
       "              'find_song': 1,\n",
       "              'song_keeps': 1,\n",
       "              'keeps_running': 1,\n",
       "              'running_head': 1})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou can't use test at the start because in production ML you don't have access to future data (e.g. test)\\nTherefore your model can't access the future word counts in test. These are unknown in production. \\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can't use test at the start because in production ML you don't have access to future data (e.g. test)\n",
    "Therefore your model can't access the future word counts in test. These are unknown in production. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int, {'awkward_use': 1, 'use_unreliable': 1}),\n",
       " defaultdict(int, {'good_hoped': 1})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_list[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11444\n"
     ]
    }
   ],
   "source": [
    "vocabulary_dict = defaultdict(int)\n",
    "\n",
    "for d in train_text_list:\n",
    "    for k,v in d.items():\n",
    "        vocabulary_dict[k] += v\n",
    "\n",
    "print(len(vocabulary_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I will use log-normalization because of the sparseness of the data.\n",
    "I want to Laplace smooth the counts for naive bayes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2400x13000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12423 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "import hashlib \n",
    "\n",
    "hash_word_dict = {}\n",
    "MAX_COLUMNS = 13000 # 22430 #9999 #2147483646 # possible hash collisions\n",
    "data_list = []\n",
    "row_ind_list = []\n",
    "col_ind_list = []\n",
    "\n",
    "for i,d in enumerate(train_text_list):\n",
    "    for k,v in d.items():\n",
    "        row_ind_list.append(i)\n",
    "        data_list.append(np.log1p(v))\n",
    "        val = int(hashlib.sha512(k.encode()).hexdigest(),16) % MAX_COLUMNS\n",
    "        hash_word_dict[val] = k\n",
    "        col_ind_list.append(val)\n",
    "#You can create csr_matrix at once (like this format: csr_matrix((data, (row_ind, col_ind))). \n",
    "X_train = sp.csr_matrix((data_list, (row_ind_list, col_ind_list))) # sparse csr matrix\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<600x13000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 347 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_COLUMNS = 13000 \n",
    "data_list = []\n",
    "row_ind_list = []\n",
    "col_ind_list = []\n",
    "\n",
    "for i,d in enumerate(test_text_list):\n",
    "    for k,v in d.items():\n",
    "        if vocabulary_dict[k] > 0:\n",
    "            row_ind_list.append(i)\n",
    "            data_list.append(np.log1p(v))\n",
    "            val = int(hashlib.sha512(k.encode()).hexdigest(),16) % MAX_COLUMNS\n",
    "            col_ind_list.append(val)\n",
    "#You can create csr_matrix at once (like this format: csr_matrix((data, (row_ind, col_ind))). \n",
    "X_test = sp.csr_matrix((data_list, (row_ind_list, col_ind_list))) # sparse csr matrix\n",
    "X_test = sp.csr_matrix((X_test.data, X_test.indices, X_test.indptr), shape=(600, MAX_COLUMNS))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# The difference between MultinomialNB and my_naive_bayes is \n",
    "# that sklearn will np.log1p again within its MultinomialNB\n",
    "# I only np.log1p once in preprocessing\n",
    "clf = MultinomialNB(alpha=0.00000001)\n",
    "clf.fit(X_train, train_score_list)\n",
    "preds = clf.predict(X_test)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[271,  29],\n",
       "       [205,  95]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_val =  confusion_matrix(test_score_list, preds)\n",
    "cm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "\n",
    "#afe_sparse_dot(X, self.feature_log_prob_.T) \n",
    "def my_naive_bayes():\n",
    "    \"\"\"\n",
    "    Scipy implementation of MultinomialNB with equal class distribution assumed. \n",
    "    Our example in the HW2 has equal class distribution 1:1\n",
    "    \"\"\"\n",
    "    y = sp.csr_matrix(([1]*len(train_score_list), (list(range(len(train_score_list))),\n",
    "                                           train_score_list)))\n",
    "\n",
    "    feature_count = y.T.dot(X_train)\n",
    "    preds =  feature_count.dot(X_test.T)\n",
    "    \n",
    "    # If class distribution was not 1:1\n",
    "    #print(preds.shape)\n",
    "    mean_val = np.mean(train_score_list)\n",
    "    if mean_val != 0.5:\n",
    "        class_prior = np.vstack((np.array([1-mean_val]*preds.shape[1]),\n",
    "                                np.array([mean_val]*preds.shape[1])))\n",
    "        #print(class_prior.shape)\n",
    "        preds += np.log1p(class_prior.reshape(2,-1))\n",
    "    \n",
    "    \n",
    "    preds = np.argmax(preds, axis=0)\n",
    "    return preds.tolist()[0]\n",
    "\n",
    "preds = my_naive_bayes()\n",
    "preds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[272,  28],\n",
       "       [206,  94]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_val =  confusion_matrix(test_score_list, preds)\n",
    "cm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MyNaiveBayes Test accuracy of 0.61\n",
    "np.sum(np.array(test_score_list)== np.array(preds))/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5733333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[285,  15],\n",
       "       [241,  59]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty='elasticnet',l1_ratio=0.5, solver='saga')\n",
    "clf.fit(X_train, train_score_list)\n",
    "preds = clf.predict(X_test)\n",
    "print(np.sum(np.array(test_score_list)== np.array(preds))/len(preds))\n",
    "#0.5733333333333334\n",
    "cm_val =  confusion_matrix(test_score_list, preds)\n",
    "cm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'works_great'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'works_great'\n",
    "hash_word_dict[np.argmax(clf.coef_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'waste_time'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'waste_time'\n",
    "hash_word_dict[np.argmin(clf.coef_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive tuples like works_great make a label great, Negative phrases such as waste_time make a label bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLogistic Regression Elasticnet l1 ratio 0.5 performs the best with 1-grams. \\nIt appears 2-grams are worse given the small data row size. \\nPositive words and phrases make a online review positive. Great.\\nNegative words and phrases make a online review negative. Bad. Waste.\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Logistic Regression Elasticnet l1 ratio 0.5 performs the best with 1-grams. \n",
    "It appears 2-grams are worse given the small data row size. \n",
    "Positive words and phrases make a online review positive. Great.\n",
    "Negative words and phrases make a online review negative. Bad. Waste.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
